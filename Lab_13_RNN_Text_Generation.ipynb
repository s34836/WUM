{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s34836/WUM/blob/main/Lab_13_RNN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "334f24ea",
      "metadata": {
        "id": "334f24ea"
      },
      "source": [
        "# RNN Text Generation\n",
        "\n",
        "## Tasks\n",
        "\n",
        "The `shakespeare.txt` file contains a sample of Shakespeare's works. Use the data to train an RNN model to generate similar text by predicting the next character.\n",
        "\n",
        "The code below encodes the text and splits it into sequences of 100 charcters (where the first 99 characters are the input and the last character is the target).\n",
        "\n",
        "1. Create network with an Embedding layer, a GRU layer and a Dense output layer and train it on the prepared dataset.\n",
        "2. Use the model to generate new text based on a seed  (you can use the one provided below). Encode the seed sequence and pass it as input to the model and predict the next character. Then append the character to the sequence and pass it back to the model. Repeat the process to generate a given number of characters. Then decode the generated sequence.\n",
        "\n",
        "Hint: The model should output a probability distribution generated by the softmax function. You will get better results if you sample from the distribution instead of always predicting the maximum-probability character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7ea1ba8",
      "metadata": {
        "id": "f7ea1ba8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "vocab = list(set(shakespeare_text))\n",
        "vocab_size = len(vocab)\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "def encode(text):\n",
        "    return np.array([char_to_index[char] for char in text if char in char_to_index])\n",
        "\n",
        "def decode(indices):\n",
        "    return ''.join(vocab[i] for i in indices if i < vocab_size)\n",
        "\n",
        "encoded_text = encode(shakespeare_text)\n",
        "encoded_text_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "sequence_length = 100\n",
        "sequences = encoded_text_dataset.batch(sequence_length, drop_remainder=True)\n",
        "\n",
        "dataset = sequences.map(lambda seq: (seq[:-1], seq[-1]))\n",
        "\n",
        "batch_size = 64\n",
        "dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b3e10693",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3e10693",
        "outputId": "55d894bf-b090-4e26-b1e4-64fe1b68b7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence:\n",
            "put on their cloaks;\n",
            "When great leaves fall, the winter is at hand;\n",
            "When the sun sets, who doth not\n",
            "\n",
            "Target:  \n"
          ]
        }
      ],
      "source": [
        "for input, target in dataset.unbatch().take(1):\n",
        "    print(\"Input sequence:\")\n",
        "    print(decode(input.numpy()))\n",
        "    print(\"\\nTarget:\", decode([target.numpy()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31f50d21",
      "metadata": {
        "id": "31f50d21"
      },
      "outputs": [],
      "source": [
        "seed = \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 64\n",
        "rnn_units = 256\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(sequence_length - 1,), dtype=tf.int32),\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "dCRLJSFE4DYL",
        "outputId": "fc85739c-b461-4c9e-c703-6fe405c70a28"
      },
      "id": "dCRLJSFE4DYL",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m247,296\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)             │        \u001b[38;5;34m16,705\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">247,296</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,705</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m268,161\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,161</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,161\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,161</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfDCkDic4GbI",
        "outputId": "955f0aa0-704b-4573-ff27-056fc9e999d9"
      },
      "id": "rfDCkDic4GbI",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 423ms/step - accuracy: 0.1533 - loss: 3.4147\n",
            "Epoch 2/5\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 467ms/step - accuracy: 0.2828 - loss: 2.5658\n",
            "Epoch 3/5\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 407ms/step - accuracy: 0.3105 - loss: 2.3878\n",
            "Epoch 4/5\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 415ms/step - accuracy: 0.3296 - loss: 2.3206\n",
            "Epoch 5/5\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 419ms/step - accuracy: 0.3456 - loss: 2.2292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_text(model, seed_text, num_chars, temperature):\n",
        "    # encode seed (ignoruje znaki spoza vocab)\n",
        "    seed_idx = encode(seed_text)\n",
        "    if len(seed_idx) == 0:\n",
        "        raise ValueError(\"Seed po enkodowaniu jest pusty (same znaki spoza vocab).\")\n",
        "\n",
        "    generated = list(seed_idx)\n",
        "\n",
        "    for _ in range(num_chars):\n",
        "        # bierzemy ostatnie 99 znaków jako input; jak seed krótszy to dopadujemy zerami\n",
        "        x = np.array(generated[-(sequence_length - 1):], dtype=np.int32)\n",
        "        if len(x) < (sequence_length - 1):\n",
        "            x = np.pad(x, (sequence_length - 1 - len(x), 0), mode=\"constant\", constant_values=0)\n",
        "\n",
        "        x = x.reshape(1, -1)\n",
        "\n",
        "        probs = model.predict(x, verbose=0)[0]  # (vocab_size,)\n",
        "\n",
        "        # temperatura: logits = log(p), skalowanie, softmax\n",
        "        probs = np.asarray(probs).astype(np.float64)\n",
        "        probs = np.log(probs + 1e-9) / temperature\n",
        "        probs = np.exp(probs)\n",
        "        probs = probs / np.sum(probs)\n",
        "\n",
        "        next_idx = np.random.choice(vocab_size, p=probs)\n",
        "        #next_idx = np.argmax(probs)\n",
        "        generated.append(next_idx)\n",
        "\n",
        "    return decode(generated)\n",
        "\n",
        "print(generate_text(model, seed, num_chars=300, temperature=1.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsvSjThV4I7I",
        "outputId": "34494d81-2ec7-413e-f2a9-edfac9149a00"
      },
      "id": "UsvSjThV4I7I",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be, that is the question:\n",
            "Whether 'tis nobler in the mind to suffer\n",
            "The slings and arrows of outrageous fortune,\n",
            "Or to take arms against a sea of troubles\n",
            "And by opposing end them. To dieto sleep,\n",
            "No more; and by a sleep to say we end\n",
            "Cpaml sheliu thous, sacl, nost, bomy teektel!\n",
            "Bey tale. I'ccod-oked. Cely an I wormarecr.\n",
            "\n",
            "KmeSI Gror:\n",
            "tfive fel. WI Romk, ceres\n",
            "Be livy ond fnowline. -ut; dounlpd pridednar sextisct, and you.\n",
            "\n",
            "MTIOU:\n",
            "O tanD on int beqwit thy ve dtislle\n",
            "On,\n",
            "Keole mist\n",
            "mndes selle'n! the rure, g ofcliwhenc:\n",
            "And in be\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}