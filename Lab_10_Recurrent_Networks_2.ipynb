{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s34836/WUM/blob/main/Lab_10_Recurrent_Networks_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d36323e",
      "metadata": {
        "id": "6d36323e"
      },
      "source": [
        "# Lab 10 - Recurrent Networks 2\n",
        "## Text vectorization\n",
        "\n",
        "Text data can be vectorized using `TextVectorization`.\n",
        "\n",
        "### \"Bag-of-words\" models\n",
        "- Binary encoding: the text is encoded as a binary vector, in which the value 1 on the i-th position means that the text contains the i-th word in the generated dictionary (`output_mode=\"multi_hot\"`)\n",
        "- Frequency encoding: the i-th position contains the frequency of the i-th word. (`output_mode=\"count\"`)\n",
        "- It is possible to encode n-grams (sequences of n words). (e.g. `ngrams=2`)\n",
        "- Frequencies can be normalized using TF-IDF. (`output_mode=\"tf_idf\"`)\n",
        "\n",
        "## Sequential model\n",
        "\n",
        "- Texts can be encoded as sequences of numbers `output=\"int\"`.\n",
        "- The numbers in each sequence can be vectorized using one-hot encoding or with an `Embedding` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e2476a",
      "metadata": {
        "id": "61e2476a"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1. The `aclImdb` dataset contains examples of positive (`pos`) and negative (`neg`) movie reviews. Load the dataset using the `text_dataset_from_directory()` method.\n",
        "2. Create a `TextVectorization` layer with `output_mode='multi_hot'`/`'count'`/`'tf_idf'` and generate the vocabulary using the `adapt()` method.\n",
        "3. Create a dense network to classify the reviews. Insert the vectorization layer directly after the input layer.\n",
        "4. Create a `TextVectorization` layer with `output_mode='int'` and generate the vocabulary using the `adapt()` method.\n",
        "5. Create an LSTM network to classify the reviews. Insert the vectorization layer directly after the input layer, followed by an `Embedding` layer.\n",
        "6. Compare several recurrent networks with different architectures. You can use GRU layers instead of LSTM layers. Rcurrent layers can be regularized by setting the `recurrent_dropout` parameter. Plot the learning curves on training/validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3497aa9",
      "metadata": {
        "id": "d3497aa9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06121b33",
      "metadata": {
        "id": "06121b33"
      },
      "source": [
        "### Dense network (example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaad723e",
      "metadata": {
        "id": "eaad723e"
      },
      "outputs": [],
      "source": [
        "max_tokens = 5000\n",
        "\n",
        "text_vectorization = tf.keras.layers.TextVectorization(output_mode=\"multi_hot\", max_tokens=max_tokens)\n",
        "text_vectorization.adapt(train_data.map(lambda x, y: x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d1d02a",
      "metadata": {
        "id": "45d1d02a"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,), dtype=\"string\"),\n",
        "    text_vectorization,\n",
        "    # Add your layers here\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739df33e",
      "metadata": {
        "id": "739df33e"
      },
      "source": [
        "### Recurrent network (example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837c83dc",
      "metadata": {
        "id": "837c83dc"
      },
      "outputs": [],
      "source": [
        "max_tokens = 5000\n",
        "max_len = 600\n",
        "\n",
        "text_vectorization = tf.keras.layers.TextVectorization(output_mode=\"int\", max_tokens=max_tokens, output_sequence_length=max_len)\n",
        "text_vectorization.adapt(train_data.map(lambda x, y: x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94e1c5b",
      "metadata": {
        "id": "e94e1c5b"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,), dtype=\"string\"),\n",
        "    text_vectorization,\n",
        "    tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=128),\n",
        "    # Add your layers here\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}