{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s34836/WUM/blob/main/Lab_11_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbe18c5",
      "metadata": {
        "id": "4cbe18c5"
      },
      "source": [
        "# Transfer Learning\n",
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf imagenette2.tgz"
      ],
      "metadata": {
        "id": "deByqBnCUDe0"
      },
      "id": "deByqBnCUDe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "(train_ds, val_ds), ds_info = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "metadata": {
        "id": "yTGMOBhcUwtW"
      },
      "id": "yTGMOBhcUwtW",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "def prepare(image, label):\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    image = preprocess(image)\n",
        "    return image, label\n",
        "\n",
        "train = (\n",
        "    train_ds\n",
        "    .map(prepare, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(1000)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "valid = (\n",
        "    val_ds\n",
        "    .map(prepare, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "9RdOq90wVfsy"
      },
      "id": "9RdOq90wVfsy",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f20e08",
      "metadata": {
        "id": "53f20e08"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, validation_split=0.2)\n",
        "\n",
        "train = data_generator.flow_from_directory(\n",
        "    \"kagglecatsanddogs_5340/PetImages\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training')\n",
        "\n",
        "valid = data_generator.flow_from_directory(\n",
        "    \"kagglecatsanddogs_5340/PetImages\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc5472d",
      "metadata": {
        "id": "abc5472d"
      },
      "outputs": [],
      "source": [
        "image_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f225ccc",
      "metadata": {
        "id": "8f225ccc"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(shape=image_shape),\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdc475f",
      "metadata": {
        "id": "4fdc475f"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2526de0",
      "metadata": {
        "id": "d2526de0"
      },
      "outputs": [],
      "source": [
        "model.fit(train, validation_data=valid, batch_size=32, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac1312f9",
      "metadata": {
        "id": "ac1312f9"
      },
      "source": [
        "## Tasks\n",
        "1. Use one of the pretrained models available in Tensorflow to classiy images in the `imagenette2` dataset. See the list of available models [here](https://keras.io/api/applications/).\n",
        "2. (optional) Fine-tune the pretrained model. Unfreeze the last few convolutional layers of the model trained in Task 1 by setting `trainable=True`. Then recompile the model and train it for a few more epochs with a low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# --- ustawienia\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "train_dir = \"imagenette2/train\"\n",
        "val_dir   = \"imagenette2/val\"\n",
        "\n",
        "# --- generatory (preprocess POD MobileNetV2)\n",
        "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",  # Imagenette ma 10 klas\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_ds = val_gen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- pretrained backbone\n",
        "base = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(image_size[0], image_size[1], 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "base.trainable = False  # najpierw trenujemy tylko head\n",
        "\n",
        "# --- model\n",
        "inputs = tf.keras.Input(shape=(image_size[0], image_size[1], 3))\n",
        "x = base(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(train_ds.num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "\n",
        "# --- opcjonalny fine-tuning: odmroź kawałek backbone\n",
        "base.trainable = True\n",
        "for layer in base.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=5)"
      ],
      "metadata": {
        "id": "hntYfaYJWNws"
      },
      "id": "hntYfaYJWNws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# --- ustawienia\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "train_dir = \"imagenette2/train\"\n",
        "val_dir   = \"imagenette2/val\"\n",
        "\n",
        "# --- generatory (preprocess POD EfficientNetV2)\n",
        "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",  # Imagenette ma 10 klas\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_ds = val_gen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- pretrained backbone\n",
        "base = tf.keras.applications.EfficientNetV2B0(\n",
        "    input_shape=(image_size[0], image_size[1], 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "base.trainable = False  # najpierw trenujemy tylko head\n",
        "\n",
        "# --- model\n",
        "inputs = tf.keras.Input(shape=(image_size[0], image_size[1], 3))\n",
        "x = base(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(train_ds.num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "\n",
        "# --- opcjonalny fine-tuning: odmroź kawałek backbone\n",
        "base.trainable = True\n",
        "for layer in base.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=5)"
      ],
      "metadata": {
        "id": "Q4tIQC2Ucjkt"
      },
      "id": "Q4tIQC2Ucjkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1) Odmroź ostatnie kilka warstw KONWOLUCYJNYCH (Conv2D / DepthwiseConv2D)\n",
        "#    Resztę zostaw zamrożoną.\n",
        "def unfreeze_last_conv_layers(base_model, n_conv_layers_to_unfreeze=20):\n",
        "    conv_like = (tf.keras.layers.Conv2D, tf.keras.layers.DepthwiseConv2D)\n",
        "    conv_layers = [l for l in base_model.layers if isinstance(l, conv_like)]\n",
        "\n",
        "    # nic nie odmrażamy jeśli jest za mało warstw\n",
        "    n = min(n_conv_layers_to_unfreeze, len(conv_layers))\n",
        "    to_unfreeze = set(conv_layers[-n:])\n",
        "\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers:\n",
        "        # odmrażamy tylko wybrane convy\n",
        "        layer.trainable = (layer in to_unfreeze)\n",
        "\n",
        "    # opcjonalnie: wypisz ile warstw trainable\n",
        "    trainable_cnt = sum(l.trainable for l in base_model.layers)\n",
        "    print(f\"Base model layers trainable: {trainable_cnt} / {len(base_model.layers)}\")\n",
        "\n",
        "# base to Twój EfficientNetV2B0 z wcześniejszego kodu\n",
        "unfreeze_last_conv_layers(base, n_conv_layers_to_unfreeze=20)\n",
        "\n",
        "# 2) Recompile z niskim learning rate (kluczowy krok)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# 3) Trenuj kilka epok + sensowne callbacki\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\", patience=2, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=1, min_lr=1e-7\n",
        "    )\n",
        "]\n",
        "\n",
        "history_ft = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "c-WVRR41eBvo"
      },
      "id": "c-WVRR41eBvo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}