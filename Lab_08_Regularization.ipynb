{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s34836/WUM/blob/main/Lab_08_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3778cad2",
      "metadata": {
        "id": "3778cad2"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "## Early stopping\n",
        "\n",
        "Early stopping can be implemented by adding a callback that will stop the training process when a given metric stops improving.\n",
        "\n",
        "```python\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=2, restore_best_weights=True, verbose=1)\n",
        "\n",
        "model.fit(..., callbacks=[early_stopping])\n",
        "```\n",
        "\n",
        "## L1/L2 Regularization\n",
        "\n",
        "L1/L2 regularization adds a penalty to the loss function based on the magnitude of the weights:\n",
        "\n",
        "$$ \\tilde{J}(W) = J(W) + \\lambda \\sum_i w_i^2 $$\n",
        "$$ \\tilde{J}(W) = J(W) + \\lambda \\sum_i |w_i| $$\n",
        "\n",
        "To apply this method, set the `kernel_regularizer` parameter, for example:\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Dense(..., kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "```\n",
        "\n",
        "## Dropout Regularization\n",
        "\n",
        "Dropout regularization works by randomly disabling the outputs of some neurons during training.\n",
        "It can be implemented by adding `Dropout` layers directly after a given layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0c9215",
      "metadata": {
        "id": "8f0c9215"
      },
      "source": [
        "## Tasks\n",
        "1. Load the `imdb` dataset from Lab 04. Create a mult-layer network to classify the data (you can use the example). Create a plot showing the loss function on training/validation data in each epoch and observe how the network overfits. Then train the network again using early stopping to stop the training process, when the validation loss stops decreasing.\n",
        "2. Add L1/L2 regularization to the network created in Task 1. Select an appropriate regularization coefficient and type. Train the network without early stopping and plot learning curve. Compare this network with the one created in Task 1.\n",
        "3. Add Dropout regularization to the network created in Task 1. Select the appropriate dropout rate. Draw the learning curve and compare the network with those from Tasks 1 and 2.\n",
        "4. Add dropout regularization to a selected convolutional network from Lab 07. Compare how the network works with/without regularization."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}